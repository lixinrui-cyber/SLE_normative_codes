{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfe57eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# 设置随机种子以获得可重复的结果\n",
    "np.random.seed(0)\n",
    "\n",
    "# 读取CSV文件\n",
    "df = pd.read_csv('D:/yanyi/PCNtoolkit-demo-main/data/BLR60/Zsumpos.csv', header=None)  # 替换为实际文件路径\n",
    "\n",
    "# 循环三次执行随机化过程并保存新文件\n",
    "for i in range(10000):\n",
    "    # 生成45个'HC'和106个'SLE'标签\n",
    "    hc_tags = np.repeat('HC', 63)\n",
    "    sle_tags = np.repeat('SLE', 124)\n",
    "    all_tags = np.concatenate((hc_tags, sle_tags))\n",
    "\n",
    "    # 打乱标签顺序\n",
    "    np.random.shuffle(all_tags)\n",
    "\n",
    "    # 将打乱后的标签作为新列添加到DataFrame中\n",
    "    df['Tag'] = all_tags\n",
    "\n",
    "    # 指定新的文件保存路径\n",
    "    new_file_path = 'D:\\\\yanyi\\\\perm\\\\60BLRpos\\\\your_modified_file_{}.csv'.format(i+1)\n",
    "\n",
    "    # 确保目录存在，如果不存在则创建\n",
    "    os.makedirs(os.path.dirname(new_file_path), exist_ok=True)\n",
    "\n",
    "    # 保存修改后的DataFrame到新的CSV文件\n",
    "    df.to_csv(new_file_path, index=False, header=False)\n",
    "\n",
    "    # 移除当前的'Tag'列，以便下一次循环可以重新添加新的随机标签\n",
    "    df.drop(columns=['Tag'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2efacb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# 指定原始文件夹和结果文件夹路径\n",
    "source_folder = 'D:/yanyi/perm/60BLRpos'\n",
    "destination_folder = 'D:/yanyi/perm/60BLRpos2'\n",
    "\n",
    "# 如果目标文件夹不存在，则创建它\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)\n",
    "\n",
    "# 获取原始文件夹中所有的CSV文件路径\n",
    "csv_files = glob.glob(os.path.join(source_folder, '*.csv'))\n",
    "\n",
    "# 遍历所有CSV文件\n",
    "for file_path in csv_files:\n",
    "    # 读取CSV文件\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    \n",
    "    # 计算最后一列是\"SLE\"的行的平均值\n",
    "    sle_averages = df[df.iloc[:, -1] == 'SLE'].iloc[:, :-1].mean(axis=0, skipna=True)\n",
    "    \n",
    "    # 计算最后一列是\"HC\"的行的平均值\n",
    "    hc_averages = df[df.iloc[:, -1] == 'HC'].iloc[:, :-1].mean(axis=0, skipna=True)\n",
    "    \n",
    "    # 创建一个新的DataFrame来保存平均值\n",
    "    averages_df = pd.DataFrame({\n",
    "        'SLE Averages': sle_averages,\n",
    "        'HC Averages': hc_averages\n",
    "    })\n",
    "    \n",
    "    # 将平均值作为新列添加到原始DataFrame的旁边\n",
    "    final_df = pd.concat([df, averages_df], axis=1)\n",
    "    \n",
    "    # 构造新的文件名和路径\n",
    "    new_file_name = os.path.basename(file_path)\n",
    "    new_file_path = os.path.join(destination_folder, new_file_name)\n",
    "    \n",
    "    # 保存修改后的DataFrame到新的CSV文件\n",
    "    final_df.to_csv(new_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0f2aca97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: glob2 in c:\\programdata\\anaconda3\\lib\\site-packages (0.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install glob2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f69f5056",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# 指定文件夹路径\n",
    "folder_path = 'D:/yanyi/perm/60BLRpos2'\n",
    "\n",
    "# 获取文件夹下所有的CSV文件路径\n",
    "file_paths = glob.glob(os.path.join(folder_path, '*.csv'))\n",
    "\n",
    "# 初始化一个空的DataFrame来存储合并后的数据\n",
    "combined_df = pd.DataFrame()\n",
    "\n",
    "# 遍历所有文件路径\n",
    "for file_path in file_paths:\n",
    "    # 读取当前文件\n",
    "    df = pd.read_csv(file_path, header=None)\n",
    "    \n",
    "    # 选择第149列，索引为148\n",
    "    sle_averages = df.iloc[:, 149]  # 使用.iloc来选择特定的列\n",
    "    \n",
    "    # 将当前文件的'SLE Averages'列添加到combined_df中\n",
    "    # 使用pd.concat来水平方向上合并，axis=1表示按列合并\n",
    "    combined_df = pd.concat([combined_df, pd.DataFrame(sle_averages)], axis=1)\n",
    "\n",
    "# 保存合并后的DataFrame到CSV文件\n",
    "combined_df.to_csv('D:/yanyi/perm/60BLRresultpos/combined_sle_average.csv', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e82dcaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000行 x 148列的随机减法结果已保存至 'D:/yanyi/perm/resultdelta/random_diffs60HCBLRpos.csv' 文件。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 假设CSV文件的路径分别是 'file1.csv' 和 'file2.csv'\n",
    "file_path1 = 'D:/yanyi/perm/60BLRresultpos/combined_hc_average.csv'\n",
    "file_path2 = 'D:/yanyi/perm/60BLRresultpos/combined_sle_average.csv'\n",
    "\n",
    "# 读取CSV文件，不将任何行作为列头\n",
    "data1 = pd.read_csv(file_path1, header=None)\n",
    "data2 = pd.read_csv(file_path2, header=None)\n",
    "\n",
    "# 确保两个CSV文件具有相同数量的行\n",
    "if len(data1) != len(data2):\n",
    "    raise ValueError(\"两个CSV文件的行数必须相同。\")\n",
    "\n",
    "num_rows = len(data1)  # 文件的行数\n",
    "\n",
    "# 初始化一个空的DataFrame，用于存储结果\n",
    "# 结果DataFrame将有10000行和与data1相同列数的列\n",
    "results_df = pd.DataFrame(np.zeros((10000, num_rows)), dtype=float)\n",
    "\n",
    "# 对每一行数据执行随机减法操作\n",
    "for i in range(num_rows):\n",
    "    row1 = data1.iloc[i]\n",
    "    row2 = data2.iloc[i]\n",
    "    \n",
    "    # 进行10000次随机选择和减法操作\n",
    "    results = []\n",
    "    for _ in range(10000):\n",
    "        idx1 = np.random.randint(0, len(row1))\n",
    "        idx2 = np.random.randint(0, len(row2))\n",
    "        result = row2.iloc[idx1] - row1.iloc[idx2]\n",
    "        results.append(result)\n",
    "    # 将当前行的结果作为一个新的列添加到DataFrame中\n",
    "    results_df.iloc[:, i] = results\n",
    "\n",
    "# 将结果DataFrame写入新的 CSV 文件，不包含行标题和列标题\n",
    "output_csv_path = 'D:/yanyi/perm/resultdelta/random_diffs60HCBLRpos.csv'\n",
    "results_df.to_csv(output_csv_path, index=False, header=False)\n",
    "\n",
    "print(f\"10000行 x {num_rows}列的随机减法结果已保存至 '{output_csv_path}' 文件。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "47178419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "每一列大于给定阈值的数据个数已保存至 'D:/yanyi/perm/resultdelta/comparison_results60HCBLRpos.csv' 文件。\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 假设CSV文件的路径分别是 'data1.csv' 和 'data2.csv'\n",
    "file_path1 = 'D:/yanyi/perm/resultdelta/random_diffs60HCBLRpos.csv'\n",
    "file_path2 = 'D:/yanyi/perm/60BLRresultpos/pos_realdelta.csv'\n",
    "\n",
    "# 读取CSV文件\n",
    "data1 = pd.read_csv(file_path1,header=None)\n",
    "data2 = pd.read_csv(file_path2,header=None)\n",
    "\n",
    "# 确保两个CSV文件都有148列\n",
    "if data1.shape[1] != 148 or data2.shape[1] != 148:\n",
    "    raise ValueError(\"CSV文件必须各有148列。\")\n",
    "\n",
    "# 初始化一个空的DataFrame，用于存储结果\n",
    "results_df = pd.DataFrame(index=range(148), columns=['GreaterThanCount'])\n",
    "\n",
    "# 对每一列数据执行操作\n",
    "for col_idx in range(148):\n",
    "    # 获取data2中对应列的值\n",
    "    threshold = data2.iloc[0, col_idx]\n",
    "    \n",
    "    # 计算data1中大于threshold的值的数量\n",
    "    greater_than_count = (data1.iloc[:, col_idx] >= threshold).sum()\n",
    "    \n",
    "    # 将结果存储到DataFrame中\n",
    "    results_df.at[col_idx, 'GreaterThanCount'] = greater_than_count\n",
    "\n",
    "# 将结果DataFrame写入新的 CSV 文件\n",
    "output_csv_path = 'D:/yanyi/perm/resultdelta/comparison_results60HCBLRpos.csv'\n",
    "results_df.to_csv(output_csv_path, index=True)\n",
    "\n",
    "print(f\"每一列大于给定阈值的数据个数已保存至 '{output_csv_path}' 文件。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae6e4042",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0679932  0.03731801 0.0679932  0.35875636 0.33548733 0.18616109\n",
      " 0.33330303 0.33330303 0.72324831 0.33330303 0.02663734 0.33330303\n",
      " 1.         0.35875636 0.03731801 0.01603173 0.72324831 0.78177807\n",
      " 0.56557706 0.18616109 0.33330303 0.33548733 0.07668324 0.0679932\n",
      " 0.07668324 0.10696474 0.56557706 0.1940482  0.33330303 0.02431185\n",
      " 0.72324831 1.         0.33330303 0.0679932  0.35875636 0.04913109\n",
      " 0.33548733 0.18616109 1.         0.56557706 0.72324831 0.01479852\n",
      " 0.33330303 1.         0.0679932  0.03731801 1.         0.87793081\n",
      " 0.56557706 0.56557706 0.56557706 0.01603173 0.09005057 0.00986568\n",
      " 0.56557706 0.00986568 0.28417158 0.00739926 0.07437205 0.09962575\n",
      " 0.18616109 0.00739926 0.56557706 1.         0.35875636 0.56557706\n",
      " 0.33843572 0.00986568 0.09962575 0.18616109 0.18616109 0.18616109\n",
      " 0.01603173 0.56557706 0.7241953  0.03731801 0.33330303 0.01603173\n",
      " 1.         0.07826773 0.72324831 0.18616109 0.56557706 1.\n",
      " 0.0679932  0.35875636 0.70971236 0.56557706 0.09962575 0.07946162\n",
      " 0.93634273 1.         0.35875636 0.33330303 0.35875636 0.35875636\n",
      " 0.07668324 0.1940482  0.18616109 0.09962575 0.28137742 0.01603173\n",
      " 0.93634273 0.07668324 0.0679932  1.         0.0679932  0.28417158\n",
      " 0.36150671 0.07668324 0.35875636 0.03731801 0.72324831 1.\n",
      " 0.93887153 0.00986568 1.         0.33330303 0.07048769 0.01707522\n",
      " 1.         1.         0.33330303 0.56557706 0.18616109 0.09962575\n",
      " 0.36150671 0.03731801 0.0679932  0.0679932  0.18616109 1.\n",
      " 0.03731801 0.33330303 0.09962575 0.0679932  0.18616109 0.56557706\n",
      " 0.35875636 0.33330303 0.09962575 0.09962575 0.02959704 0.35875636\n",
      " 0.0679932  0.09962575 0.04501216 0.93634273]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "\n",
    "# 假设你已经有了一个包含148个p值的列表\n",
    "p_values = [0.01639836,0.00579942,0.01619838,0.239576042,0.202679732,0.077092291,0.195480452,0.194980502,0.615738426,0.198180182,0.00269973,0.184881512,1,0.237776222,0.00479952,0.00129987,0.615338466,0.676132387,0.412658734,0.083391661,0.193780622,0.206279372,0.02179782,0.01509849,0.02249775,0.04119588,0.445855414,0.093090691,0.196680332,0.00229977,0.601139886,1,0.188081192,0.0169983,0.249675032,0.00829917,0.205879412,0.077892211,1,0.434656534,0.611438856,0.00069993,0.185581442,1,0.01509849,0.00469953,1,0.765223478,0.420957904,0.440755924,0.442255774,0.00109989,0.02859714,0.00019998,0.454054595,0.00039996,0.141785821,9.999E-05,0.01959804,0.03529647,0.086291371,9.999E-05,0.454754525,1,0.245475452,0.435656434,0.210378962,0.00039996,0.03529647,0.086791321,0.084991501,0.084691531,0.00119988,0.453054695,0.621437856,0.00579942,0.191980802,0.00089991,1,0.02379762,0.605339466,0.076792321,0.444455554,1,0.01529847,0.247575242,0.575442456,0.412158784,0.03769623,0.02469753,0.834516548,1,0.244175582,0.187581242,0.248675132,0.228877112,0.02089791,0.091790821,0.080391961,0.0349965,0.136886311,0.00129987,0.835116488,0.02159784,0.01669833,1,0.01309869,0.142085791,0.256474353,0.02279772,0.240275972,0.00509949,0.611238876,1,0.843715628,0.00029997,1,0.191180882,0.01809819,0.00149985,0.96470353,0.938006199,0.189281072,0.441555844,0.082491751,0.03559644,0.254074593,0.00519948,0.01629837,0.01549845,0.081791821,1,0.00569943,0.190080992,0.03679632,0.01509849,0.078492151,0.441955804,0.238176182,0.191880812,0.03769623,0.03489651,0.00319968,0.241675832,0.01539846,0.03679632,0.00729927,0.831416858]\n",
    "# 将p值列表转换为NumPy数组\n",
    "p_values_array = np.array(p_values)\n",
    "\n",
    "# 使用Benjamini-Hochberg程序进行多重比较校正\n",
    "_, corrected_p_values, _, _ = multipletests(p_values_array, method='fdr_bh')\n",
    "\n",
    "# 输出校正后的p值\n",
    "print(corrected_p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2feed4a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
